{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 구현에 필요한 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용할 게임 환경 설정 & Hyper parameter 정의하기\n",
    "\n",
    "게임 환경 : CartPole-v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "\n",
    "LR = 0.01  # learning rate\n",
    "GAMMA = 0.99  # Reward discount\n",
    "LOG_INTERVAL = 10  # Interval between trainging status logs\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Neural Network 구현\n",
    "\n",
    "모든 노드 간에 연결 되어있는 모델\n",
    "\n",
    "입력(Input) : 게임의 상태 정보\n",
    "출력(Output) : 행동에 대한 확률 값\n",
    "\n",
    "행동 선택 : 환경의 상태를 입력으로 하여 Network의 출력인 행동에 대한 확률 값을 얻음. 이 후에 확률 값을 범주화하여 행동을 선택\n",
    "\n",
    "정책 손실 함수와 보상의 곱의 합을 곱한 후 가중치를 학습(크로스 엔트로피 함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.eps = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.fc1 = nn.Linear(N_STATES, 50)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)  # Initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)  # Initialization\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        actions_policy = F.softmax(actions_value, dim=1)\n",
    "        return actions_policy\n",
    "    \n",
    "    def select_action(self, x):\n",
    "        state = torch.from_numpy(x).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def finish_episode(self):\n",
    "        R = 0\n",
    "        policy_loss = []\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + GAMMA * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        del self.rewards[:]\n",
    "        del self.saved_log_probs[:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습\n",
    "\n",
    "한 에피소드의 지정한 타임 스탭 내에서 얻은 보상을 더한 후 학습을 진행\n",
    "\n",
    "에피소드 단위로 학습을 진행한 후, running_reward가 환경의 보상 임계치를 초과할 경우 문제를 해결하며 학습을 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    policy = Policy()\n",
    "    policy.optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "    policy.eps = np.finfo(np.float32).eps.item()\n",
    "    \n",
    "    episode = []\n",
    "    re = []\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            \n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        episode.append(i_episode)\n",
    "        \n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        re.append(running_reward)\n",
    "        \n",
    "        policy.finish_episode()\n",
    "        if i_episode % LOG_INTERVAL == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "            \n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    plt.title(\"Cartpole scores for 200 episodes\")\n",
    "    plt.plot(episode, re)\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(\"avg_ep_reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
