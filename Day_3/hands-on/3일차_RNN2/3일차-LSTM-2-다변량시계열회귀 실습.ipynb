{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LSTM 실습: 다변량 시계열 예측 모델링 및 비교]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)\n",
    "- y: Code로 변경\n",
    "- m: Markdown으로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('content/gdrive/')\n",
    "import os\n",
    "os.chdir('/content/gdrove/My Drive/Day3/hands-on/3일차_RNN2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "# pip install pandas-datareader\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling function for input data\n",
    "def minmax_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 불러오기\n",
    "##### Pandas Datareader 사용: 야후에서 제공하는 API사용\n",
    "#####  \n",
    "#####  \n",
    "\n",
    "- X: 주식 정보(High, Low, Open, Close, Volumne, Adj Close)\n",
    "- y: 주식 정보(High, Low, Open, Close, Volumne, Adj Close) 중 택 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will look at stock prices over the past year\n",
    "start = (2000, 12, 1)\n",
    "start = datetime.datetime(*start) #그냥 tuple로 넣어주면 안됨, *: 인자를 각각 순서대로 넣어줌\n",
    "end = datetime.date.today()\n",
    "\n",
    "# google = pdr.DataReader('어떤종목', '어디서', 언제부터, 언제까지)                    \n",
    "yahoo = pdr.DataReader('AAPL', 'yahoo', start, end)\n",
    "\n",
    "# 한화: 000880.KS\n",
    "# 한화 케미칼: 009830.KS\n",
    "# 한화 손해보험: 000370.KS\n",
    "# 모나미: 005360.KS\n",
    "# 하이트진로홀딩스우: 000145.KS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  High         Low        Open       Close       Volume  \\\n",
      "Date                                                                      \n",
      "2000-11-30    1.214286    1.151786    1.191964    1.178571  202399400.0   \n",
      "2000-12-01    1.250000    1.200893    1.214286    1.218750   96426400.0   \n",
      "2000-12-04    1.227679    1.174107    1.227679    1.191964   92880200.0   \n",
      "2000-12-05    1.245536    1.169643    1.209821    1.214286  153494600.0   \n",
      "2000-12-06    1.071429    1.000000    1.044643    1.022321  343616000.0   \n",
      "2000-12-07    1.062500    1.000000    1.031250    1.022321  102229400.0   \n",
      "2000-12-08    1.093750    1.031250    1.058036    1.075893  108906000.0   \n",
      "2000-12-11    1.098214    1.062500    1.084821    1.084821   83127800.0   \n",
      "2000-12-12    1.142857    1.071429    1.089286    1.098214   96565000.0   \n",
      "2000-12-13    1.111607    1.062500    1.111607    1.071429   86221800.0   \n",
      "2000-12-14    1.089286    1.031250    1.073661    1.031250   65829400.0   \n",
      "2000-12-15    1.049107    1.000000    1.040179    1.004464  128486400.0   \n",
      "2000-12-18    1.044643    0.995536    1.040179    1.017857   81452000.0   \n",
      "2000-12-19    1.089286    1.000000    1.026786    1.000000   93501800.0   \n",
      "2000-12-20    1.044643    0.973214    0.984375    1.026786  141332800.0   \n",
      "2000-12-21    1.071429    0.991071    1.017857    1.004464   91711200.0   \n",
      "2000-12-22    1.071429    1.008929    1.008929    1.071429   79513000.0   \n",
      "2000-12-26    1.071429    1.017857    1.062500    1.049107   54203800.0   \n",
      "2000-12-27    1.058036    1.013393    1.024554    1.058036   81366600.0   \n",
      "2000-12-28    1.066964    1.022321    1.026786    1.058036   76294400.0   \n",
      "2000-12-29    1.071429    1.035714    1.049107    1.062500  157584000.0   \n",
      "2001-01-02    1.089286    1.040179    1.062500    1.062500  113078000.0   \n",
      "2001-01-03    1.191964    1.031250    1.035714    1.169643  204268400.0   \n",
      "2001-01-04    1.321429    1.200893    1.295759    1.218750  184849000.0   \n",
      "2001-01-05    1.241071    1.147321    1.209821    1.169643  103089000.0   \n",
      "2001-01-08    1.213170    1.138393    1.209821    1.183036   93424800.0   \n",
      "2001-01-09    1.260045    1.183036    1.200893    1.227679  147232400.0   \n",
      "2001-01-10    1.214286    1.147321    1.191964    1.183036  145195400.0   \n",
      "2001-01-11    1.321429    1.160714    1.160714    1.285714  200933600.0   \n",
      "2001-01-12    1.285714    1.218750    1.276786    1.227679  105844200.0   \n",
      "...                ...         ...         ...         ...          ...   \n",
      "2019-06-18  200.289993  195.210007  196.050003  198.449997   26551000.0   \n",
      "2019-06-19  199.880005  197.309998  199.679993  197.869995   21124200.0   \n",
      "2019-06-20  200.610001  198.029999  200.369995  199.460007   21514000.0   \n",
      "2019-06-21  200.850006  198.149994  198.800003  198.779999   47800600.0   \n",
      "2019-06-24  200.160004  198.169998  198.539993  198.580002   18220400.0   \n",
      "2019-06-25  199.259995  195.289993  198.429993  195.570007   21070300.0   \n",
      "2019-06-26  200.990005  197.350006  197.770004  199.800003   26067500.0   \n",
      "2019-06-27  201.570007  199.570007  200.289993  199.740005   20899700.0   \n",
      "2019-06-28  199.500000  197.050003  198.679993  197.919998   31110600.0   \n",
      "2019-07-01  204.490005  200.649994  203.169998  201.550003   27316700.0   \n",
      "2019-07-02  203.130005  201.360001  201.410004  202.729996   16935200.0   \n",
      "2019-07-03  204.440002  202.690002  203.279999  204.410004   11362000.0   \n",
      "2019-07-05  205.080002  202.899994  203.350006  204.229996   17265500.0   \n",
      "2019-07-08  201.399994  198.410004  200.809998  200.020004   25338600.0   \n",
      "2019-07-09  201.509995  198.809998  199.199997  201.240005   20578000.0   \n",
      "2019-07-10  203.729996  201.559998  201.850006  203.229996   17897100.0   \n",
      "2019-07-11  204.389999  201.710007  203.309998  201.750000   20191800.0   \n",
      "2019-07-12  204.000000  202.199997  202.449997  203.300003   17595200.0   \n",
      "2019-07-15  205.869995  204.000000  204.089996  205.210007   16947400.0   \n",
      "2019-07-16  206.110001  203.500000  204.589996  204.500000   16866800.0   \n",
      "2019-07-17  205.089996  203.270004  204.050003  203.350006   14107500.0   \n",
      "2019-07-18  205.880005  203.699997  204.000000  205.660004   18582200.0   \n",
      "2019-07-19  206.500000  202.360001  205.789993  202.589996   20929300.0   \n",
      "2019-07-22  207.229996  203.610001  203.649994  207.220001   22277900.0   \n",
      "2019-07-23  208.910004  207.289993  208.460007  208.839996   18355200.0   \n",
      "2019-07-24  209.149994  207.169998  207.669998  208.669998   14991600.0   \n",
      "2019-07-25  209.240005  206.729996  208.889999  207.020004   13909600.0   \n",
      "2019-07-26  209.729996  207.139999  207.479996  207.740005   17618900.0   \n",
      "2019-07-29  210.639999  208.440002  208.460007  209.679993   21560600.0   \n",
      "2019-07-30  209.589996  207.574997  208.759995  208.149994    5511037.0   \n",
      "\n",
      "             Adj Close  \n",
      "Date                    \n",
      "2000-11-30    1.032463  \n",
      "2000-12-01    1.067660  \n",
      "2000-12-04    1.044195  \n",
      "2000-12-05    1.063749  \n",
      "2000-12-06    0.895583  \n",
      "2000-12-07    0.895583  \n",
      "2000-12-08    0.942513  \n",
      "2000-12-11    0.950335  \n",
      "2000-12-12    0.962067  \n",
      "2000-12-13    0.938603  \n",
      "2000-12-14    0.903405  \n",
      "2000-12-15    0.879940  \n",
      "2000-12-18    0.891673  \n",
      "2000-12-19    0.876029  \n",
      "2000-12-20    0.899494  \n",
      "2000-12-21    0.879940  \n",
      "2000-12-22    0.938603  \n",
      "2000-12-26    0.919049  \n",
      "2000-12-27    0.926870  \n",
      "2000-12-28    0.926870  \n",
      "2000-12-29    0.930781  \n",
      "2001-01-02    0.930781  \n",
      "2001-01-03    1.024641  \n",
      "2001-01-04    1.067660  \n",
      "2001-01-05    1.024641  \n",
      "2001-01-08    1.036374  \n",
      "2001-01-09    1.075482  \n",
      "2001-01-10    1.036374  \n",
      "2001-01-11    1.126323  \n",
      "2001-01-12    1.075482  \n",
      "...                ...  \n",
      "2019-06-18  198.449997  \n",
      "2019-06-19  197.869995  \n",
      "2019-06-20  199.460007  \n",
      "2019-06-21  198.779999  \n",
      "2019-06-24  198.580002  \n",
      "2019-06-25  195.570007  \n",
      "2019-06-26  199.800003  \n",
      "2019-06-27  199.740005  \n",
      "2019-06-28  197.919998  \n",
      "2019-07-01  201.550003  \n",
      "2019-07-02  202.729996  \n",
      "2019-07-03  204.410004  \n",
      "2019-07-05  204.229996  \n",
      "2019-07-08  200.020004  \n",
      "2019-07-09  201.240005  \n",
      "2019-07-10  203.229996  \n",
      "2019-07-11  201.750000  \n",
      "2019-07-12  203.300003  \n",
      "2019-07-15  205.210007  \n",
      "2019-07-16  204.500000  \n",
      "2019-07-17  203.350006  \n",
      "2019-07-18  205.660004  \n",
      "2019-07-19  202.589996  \n",
      "2019-07-22  207.220001  \n",
      "2019-07-23  208.839996  \n",
      "2019-07-24  208.669998  \n",
      "2019-07-25  207.020004  \n",
      "2019-07-26  207.740005  \n",
      "2019-07-29  209.679993  \n",
      "2019-07-30  208.149994  \n",
      "\n",
      "[4693 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(yahoo)\n",
    "# High: 장 중 제일 높았던 주가(고가)\n",
    "# Low: 장 중 제일 낮았던 주가(저가)\n",
    "# Open: 장 시작 때 주가(시가)\n",
    "# Close: 장 닫을 때 주가(종가)\n",
    "# Volume: 주식 거래량\n",
    "# Adj Close: 주식의 분할, 배당, 배분 등을 고려해 조정한 종가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yahoo.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yahoo.Close.plot(grid=True)  # Close --> change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rvs = yahoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperparameters\n",
    "\n",
    "- seq_length: 시퀀스 길이\n",
    "- input_dim: 변수 개수\n",
    "- hidden_dim: hidden state 차원(=하나의 hidden state 내 특징을 어느 크기로 추출할 것인지)\n",
    "- layers: hidden state 개수(= 추출하는 특징을 몇개의 hidden state로 구성할 것인지)\n",
    "- output_dim: 출력 크기(회귀에서는 1, 분류에서는 클래스 개수)\n",
    "- learning_rate: 학습률\n",
    "- n_epochs: 학습 반복 회수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 7    # 반영하고자 하는 날짜\n",
    "data_dim = 6      # 변수 갯수 \n",
    "hidden_dim = 10   #\n",
    "output_dim = 1    # 예측 변수 갯수\n",
    "learning_rate = 0.01\n",
    "iterations = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train-test set\n",
    "train_size = int(len(data_rvs) * 0.7)\n",
    "train_set = data_rvs[0:train_size]\n",
    "test_set = data_rvs[train_size - seq_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data\n",
    "train_set = minmax_scaler(train_set)\n",
    "test_set = minmax_scaler(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 전처리: sequence 길이에 맞게  RNN Input 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(time_series, seq_length):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(time_series) - seq_length):\n",
    "        _x = time_series.iloc[i:i + seq_length, :]\n",
    "        _y = time_series.iloc[i + seq_length, [3]]  # Next close price  #[3,4]: Close and volume\n",
    "        print(_x, \"->\", _y)\n",
    "        dataX.append(_x.values)\n",
    "        dataY.append(_y.values)\n",
    "    \n",
    "    return np.stack(dataX), np.stack(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind\n",
    "\n",
    "#seq_length = 7    # 반영하고자 하는 날짜\n",
    "#data_dim = 6      # 변수 갯수 \n",
    "#hidden_dim = 10\n",
    "#output_dim = 1    # 예측 변수 갯수\n",
    "#learning_rate = 0.01\n",
    "#iterations = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train-test dataset to input\n",
    "trainX, trainY = build_dataset(train_set, seq_length)\n",
    "testX, testY = build_dataset(test_set, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape) #3277,7,6\n",
    "print(trainY.shape) #3277,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testX.shape) #1408,7,6\n",
    "print(testY.shape) #1408,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor\n",
    "trainX_tensor = torch.FloatTensor(trainX)\n",
    "trainY_tensor = torch.FloatTensor(trainY)\n",
    "\n",
    "testX_tensor = torch.FloatTensor(testX)\n",
    "testY_tensor = torch.FloatTensor(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LSTM & GRU 학습 및 평가: 다음 시점의 Close price 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        #self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        #self.rnn = torch.nn.GRU(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x[:, -1])\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(data_dim, hidden_dim, output_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss & optimizer setting\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "for i in range(iterations):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(trainX_tensor)\n",
    "    loss = criterion(outputs, trainY_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training\n",
    "plt.plot(trainY)\n",
    "plt.plot(net(trainX_tensor).data.numpy())\n",
    "plt.legend(['original', 'prediction'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start testing\n",
    "for i in range(iterations):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    test_outputs = net(testX_tensor)\n",
    "    loss = criterion(test_outputs, testY_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot testing\n",
    "plt.plot(testY)\n",
    "plt.plot(net(testX_tensor).data.numpy())\n",
    "plt.legend(['original', 'prediction'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve prediction y(tensor) to numpy array\n",
    "train_predictionY = outputs.detach()\n",
    "train_predictionY = train_predictionY.numpy()\n",
    "\n",
    "test_predictionY = test_outputs.detach()\n",
    "test_predictionY = test_predictionY.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(trainY, train_predictionY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(testY, test_predictionY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN\n",
    "## Test MSE: 6.535761378376095e-05\n",
    "## Train MSE: 0.00022348110553613228\n",
    "\n",
    "\n",
    "## LSTM\n",
    "## Test MSE: 6.259012494261207e-05\n",
    "## Train MSE: 0.00019432165494639008\n",
    "\n",
    "\n",
    "## GRU\n",
    "## Test MSE: 5.0685171742773086e-05\n",
    "## Train MSE: 0.00023175848398944936\n",
    "\n",
    "#### 과대적합이 발생한 상황이다.\n",
    "#### 1. 은닉층 차원 줄이기\n",
    "#### 2. 시퀀스 길이 늘리기\n",
    "#### 3. 더 작은 학습률 적용시키기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
