{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [튜토리얼2 - LSTM 모델을 활용한 IMDB 데이터 감성 예측]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)\n",
    "- y: Code로 변경\n",
    "- m: Markdown으로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모듈 불러오기\n",
    "\n",
    "#### import '불러올 패키지명' as '그 패키지를 파이썬에서 사용할 이름'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### computer vision을 다룰 때에는 torchvision제공\n",
    "#### text를 다룰 때에는 torchtext제공\n",
    "from torchtext import data,datasets\n",
    "# pip install torchtext\n",
    "\n",
    "from torchtext.vocab import GloVe,FastText,CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchtext.datasets.imdb import IMDB\n",
    "import sys\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터\n",
    "\n",
    "#### Pandas Datareader 사용: 야후에서 제공하는 API사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 데이터 불러오기\n",
    "\n",
    "#### IMDB영화 데이터셋 내려받기 \n",
    "#### https://www.kaggle.com/orgesleka/imdbmovies\n",
    "\n",
    "- X: large movie review sentence dataset\n",
    "- y: good(긍정) / bad(부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 다운\n",
    "# 토큰화 수행\n",
    "# 데이터셋을 학습,시험 데이터로 분할\n",
    "\n",
    "TEXT = data.Field(lower=True,fix_length=200,batch_first=False)  # X fix_length: sequence length\n",
    "LABEL = data.Field(sequential=False,)                           # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\imdb\\aclImdb_v1.tar.gz: 100%|███████████████████████████████████████████████| 84.1M/84.1M [00:20<00:00, 4.07MB/s]\n"
     ]
    }
   ],
   "source": [
    "train, test = IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [03:02, 4.72MB/s]                                                                    \n",
      "100%|███████████████████████████████████████████████████████████████████████▉| 399413/400000 [00:47<00:00, 8948.26it/s]"
     ]
    }
   ],
   "source": [
    "# 어휘 구축\n",
    "# 어휘 객체 생성에 train객체 전달\n",
    "'''\n",
    "- charngram.100d\n",
    "- fasttext.en.300d\n",
    "- fasttext.simple.300d\n",
    "- glove.42B.300d\n",
    "- glove.840B.300d\n",
    "- glove.twitter.27B.25d\n",
    "- glove.twitter.27B.50d\n",
    "- glove.twitter.27B.100d\n",
    "- glove.twitter.27B.200d\n",
    "- glove.6B.50d\n",
    "- glove.6B.100d\n",
    "- glove.6B.200d\n",
    "- glove.6B.300d\n",
    "'''\n",
    "\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\n",
    "LABEL.build_vocab(train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x000001EC36C5C390>, 'label': <torchtext.data.field.Field object at 0x000001EC36C5C3C8>}\n",
      "len(train) 25000\n",
      "vars(train[0]) {'text': ['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"teachers\".', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', \"high's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"teachers\".', 'the', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'i', 'immediately', 'recalled', '.........', 'at', '..........', 'high.', 'a', 'classic', 'line:', 'inspector:', \"i'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'student:', 'welcome', 'to', 'bromwell', 'high.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched.', 'what', 'a', 'pity', 'that', 'it', \"isn't!\"], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pos': 12500, 'neg': 12500})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'stoi', 'vectors'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = vars(TEXT.vocab)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.7724, -0.1800,  0.2072,  ...,  0.6736,  0.2263, -0.2919],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10002, 300])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 벡터 배치 생성하기\n",
    "\n",
    "#### 단어를 인덱스 번호로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, device=-1)\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch.text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBRnn(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab,hidden_size,n_cat,bs=1,nl=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bs = bs\n",
    "        self.nl = nl\n",
    "        self.e = nn.Embedding(n_vocab,hidden_size)               #[200,32,100]\n",
    "        self.rnn = nn.LSTM(hidden_size,hidden_size,nl)\n",
    "        self.fc2 = nn.Linear(hidden_size,n_cat)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self,inp):\n",
    "        bs = inp.size()[1]\n",
    "        if bs != self.bs:\n",
    "            self.bs = bs\n",
    "        e_out = self.e(inp)\n",
    "        h0 = c0 = Variable(e_out.data.new(*(self.nl,self.bs,self.hidden_size)).zero_())\n",
    "        rnn_o,_ = self.rnn(e_out,(h0,c0)) \n",
    "        rnn_o = rnn_o[-1]\n",
    "        fc = F.dropout(self.fc2(rnn_o),p=0.8)\n",
    "        return self.softmax(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IMDBRnn(n_vocab, n_hidden, 3, bs=32)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "def fit(epoch,model,data_loader,phase='training',volatile=False):\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if phase == 'validation':\n",
    "        model.eval()\n",
    "        volatile=True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    for batch_idx , batch in enumerate(data_loader):\n",
    "        text , target = batch.text , batch.label\n",
    "        if is_cuda:\n",
    "            text,target = text.cuda(),target.cuda()\n",
    "        \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = F.nll_loss(output,target)\n",
    "        \n",
    "        running_loss += F.nll_loss(output,target,size_average=False).data\n",
    "        preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct.item()/len(data_loader.dataset)\n",
    "    \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is   0.7 and training accuracy is 12564/25000     50.26\n",
      "validation loss is   0.7 and validation accuracy is 12500/25000      50.0\n",
      "training loss is  0.69 and training accuracy is 13159/25000     52.64\n",
      "validation loss is  0.69 and validation accuracy is 13456/25000     53.82\n",
      "training loss is  0.63 and training accuracy is 15988/25000     63.95\n",
      "validation loss is  0.51 and validation accuracy is 19222/25000     76.89\n",
      "training loss is   0.4 and training accuracy is 20712/25000     82.85\n",
      "validation loss is  0.41 and validation accuracy is 20508/25000     82.03\n"
     ]
    }
   ],
   "source": [
    "train_losses , train_accuracy = [],[]\n",
    "val_losses , val_accuracy = [],[]\n",
    "\n",
    "for epoch in range(1,5):\n",
    "\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is  0.28 and training accuracy is 22226/25000      88.9\n",
      "validation loss is  0.37 and validation accuracy is 20913/25000     83.65\n",
      "training loss is  0.21 and training accuracy is 23050/25000      92.2\n",
      "validation loss is  0.41 and validation accuracy is 20840/25000     83.36\n",
      "training loss is  0.15 and training accuracy is 23787/25000     95.15\n",
      "validation loss is  0.48 and validation accuracy is 20806/25000     83.22\n",
      "training loss is 0.097 and training accuracy is 24275/25000      97.1\n",
      "validation loss is  0.56 and validation accuracy is 20774/25000      83.1\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1,5):\n",
    "\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation loss가 크다면?\n",
    "\n",
    "#### 과대적합이 발생한 상황이다.\n",
    "#### 1. 은닉층 차원 줄이기\n",
    "#### 2. 시퀀스 길이 늘리기\n",
    "#### 3. 더 작은 학습률 적용시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
